---
title: "Assignment 1 ski data"
author: "Larissa, Ralph, Danique en Jelle"
date: "3/22/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set options
options(stringsAsFactors=F)
Sys.setlocale('LC_ALL','C')

# Loading libraries
library(stringi)
library(stringr)
library(qdap)
library(lubridate)
library(tm)
library(ggplot2)
library(wordcloud)
library(jsonlite)
library(dplyr)
library(tokenizers)
library(tidytext)
library(SnowballC)
library(corpus)
library(plotrix)
library(mgsub)
library(syuzhet)
library(ggrepel)
library(quanteda)
library(smacof)
library(factoextra)
library(readr)

# Loading data files
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

text.df <- read.csv("OnTheSnow_SkiAreaReviews.csv", comment.char="#")

reviews <- data.frame(ID=seq(1:nrow(text.df)),
                      text=text.df$Review.Text, 
                      stars=text.df$Review.Star.Rating..out.of.5.) 

```

## Stage one

Cleaning of the data, removing punctuation and if needed, stemming. Data must be stored in 2 versions:
- Version 1: The dataset without stemming and including punctuation (needed for stage three)
- Version 2: The dataset as a complete fully cleaned word vector representation.


```{r}
#Create backup for stage 3 (including punctuation): 
reviews_backup <- reviews 

#First stage of cleaning
reviews$text <- as.character(reviews$text)  %>% 
  tolower() %>% 
  {gsub(":( |-|o)*\\("," SADSMILE ", .)} %>%       # Find :( or :-( or : ( or :o(
  {gsub(":( |-|o)*\\)"," HAPPYSMILE ", .)} %>%     # Find :) or :-) or : ) or :o)
  {gsub("(\"| |\\$)-+\\.-+"," NUMBER", .)} %>%     # Find numbers
  {gsub("([0-9]+:)*[0-9]+ *am"," TIME_AM", .)} %>%         # Find time AM
  {gsub("([0-9]+:)*[0-9]+ *pm"," TIME_PM", .)} %>%         # Find time PM
  {gsub("-+:-+","TIME", .)} %>%                    # Find general time
  {gsub("\\$ ?[0-9]*[\\.,]*[0-9]+"," DOLLARVALUE ", .)} %>%           # Find Dollar values
  {gsub("[0-9]*[\\.,]*[0-9]+"," NUMBER ", .)} %>%           # Find remaining numbers
  {gsub("-"," ", .)} %>%                           # Remove all -
  {gsub("\\("," ", .)} %>%                           #Remove all (
  {gsub("\\)"," ", .)} %>%                           #Remove all )
  {gsub("\""," ", .)} %>%                           #Remove all "
  {gsub("\\,"," ", .)} %>%                            #Remove all ,
  {gsub("&"," and ", .)} %>%                       # Find general time
  {gsub("\"+"," ", .)} %>%                         # Remove all "
  {gsub("\\|+"," ", .)} %>%                        # Remove all |
  {gsub("_+"," ", .)} %>%                          # Remove all _
  {gsub(";+"," ", .)} %>%                          # Remove excess ;
  {gsub(" +"," ", .)} %>%                          # Remove excess spaces
  {gsub("\\.+","\\.", .)}                          # Remove excess .

#stemming, unnesting and removing stop words and create word vector
j <- 1
for (j in 1:nrow(reviews)) {
  stemmed_description <- anti_join((reviews[j,] %>% 
                                      unnest_tokens(word,text, drop=FALSE,to_lower=TRUE) ),stop_words)
  
  stemmed_description <- (wordStem(stemmed_description[,"word"], language = "porter"))
  
  reviews[j,"text"] <- paste((stemmed_description),collapse = " ")
  
}

reviews_vector <- reviews %>% 
                    unnest_tokens(word,text, to_lower=TRUE)

#remove very frequent and infrequent words
counts <- reviews_vector %>% 
            count(word, sort=TRUE)
infrequent <- counts %>% 
                filter(n <0.00005 * nrow(reviews_vector))
frequent <- counts[1:1,] #remove words ski and "c"
toremove <- full_join(frequent,infrequent)
reviews_vector_cleaned <- reviews_vector %>% 
                            anti_join(toremove)

save(reviews_vector_cleaned, file="cleanedwordvector.Rdata") # (2) fully cleaned word vector representation


# Creating the full review from the cleaned and stemmed words
j<-1 
for (j in 1:nrow(reviews)) {
 stemmed_Review<-  anti_join((reviews[j,] %>% 
                                unnest_tokens(word,text,to_lower=TRUE)), toremove)
  
 reviews[j,"text"]<- paste((stemmed_Review[,"word"]),collapse = " ")

}
# Remove empty reviews
reviews <- reviews[!(is.na(reviews$text) | reviews$text==""), ]

save(reviews, file="partially_cleaned_reviews.Rdata") # full sentences
```

Stage 2
MDS

```{r}
load(file= 'partially_cleaned_reviews.Rdata')

# Creating distance matrix
reviews_corp <- corpus(reviews, docid_field = "ID", text_field = "text")
co_occurrence_matrix <- fcm(x = reviews_corp, context = "document", count = "frequency", tri=FALSE) # get feature co-occurence matrix: number of co-occurences of words

# Moeten we "context = document gebruiken of context = window?" --> JELLE: document omdat we alle woordenaantallen willen pakken in de hele review (https://stackoverflow.com/questions/55509338/is-there-an-r-function-for-finding-keywords-within-a-certain-word-distance)
# Moeten we count = frequency of count = boolean gebruiken? -> JELLE: frequency geeft aan hoe vaak woorden in eenzelfde review bevinden, boolean geeft alleen ja/nee, frequency lijkt mij daarom relevanter

reviews_dfm <- dfm(reviews_corp) # get document frequency matrix: how frequent does each term appear in each document
counts <- colSums(as.matrix(reviews_dfm)) # calculate total count of documents
co_occurrence_matrix <- as.matrix(co_occurrence_matrix)
diag(co_occurrence_matrix) <- counts # adjusts diagonal counts upward (also single counts)

# take a subset of the most frequent words
sortedcount <- counts%>% sort(decreasing=TRUE)
sortednames <- names(sortedcount)
nwords<-200
subset_words<-as.matrix(sortedcount[1:nwords])

# create a subset of the original co_occurance_matrix
co_occurrence_matrix <- co_occurrence_matrix[sortednames[1:nwords],sortednames[1:nwords]]
distances <- sim2diss(co_occurrence_matrix, method = "cooccurrence") # Transform similarities to distances.
```

```{r}

MDS_map <- smacofSym(distances) # run the routine that finds the best matching coordinates in a 2D mp given the distances
ggplot(as.data.frame(MDS_map$conf), aes(D1, D2, label = rownames(MDS_map$conf))) +
     geom_text(check_overlap = TRUE) + theme_minimal(base_size = 15) + xlab('') + ylab('') +
     scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = NULL)
# the conf element in the MDS output contains the coordinatis with as names D1 and D2.
```

PCA
```{r}
review_tdm <- reviews %>% 
                  unnest_tokens(word,text) %>% 
                      count(word,ID,sort=TRUE) %>%
                        ungroup()%>%
                          cast_tdm(word,ID,n)

# take a subset of the most frequent words
counts <- rowSums(as.matrix(review_tdm)) 
sortedcount <- counts %>% 
                  sort(decreasing=TRUE)
nwords<-200
sortednames <- names(sortedcount[1:nwords])

pca_results <- prcomp(t(review_tdm[,1:10000]), scale = FALSE, rank. = 50) # why do we set scale to FALSE? -> All data is measured on the same scale 
pca_results_backup <- pca_results  # create a backup of results for later use
```

```{r}
fviz_screeplot(pca_results,ncp=30)
```

```{r}
ncomp<-5
# Hoe kies je ncomp?

j<-1
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <- (names(toplist))
for (j in 2:ncomp){
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <-cbind( topwords , (names(toplist)))
  }

topwords

```

```{r}
#PCA plot
axeslist <- c(1, 2)
fviz_pca_var(pca_results, axes=axeslist 
             ,geom.var = c("arrow", "text")
              ,col.var = "contrib", # Color by contributions to the PC
              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
              repel = TRUE     # Avoid text overlapping
             )


```


