---
title: "Assignment 1 ski data"
author: "Larissa, Ralph, Danique en Jelle"
date: "3/22/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set options
options(stringsAsFactors=F)
Sys.setlocale('LC_ALL','C')

# Loading libraries
library(stringi)
library(stringr)
library(qdap)
library(lubridate)
library(tm)
library(ggplot2)
library(wordcloud)
library(jsonlite)
library(dplyr)
library(tokenizers)
library(tidytext)
library(SnowballC)
library(corpus)
library(plotrix)
library(mgsub)
library(syuzhet)
library(ggrepel)
library(quanteda)
library(smacof)
library(factoextra)
library(readr)

# Loading data files
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

text.df <- read.csv("OnTheSnow_SkiAreaReviews.csv", comment.char="#")

reviews <- data.frame(ID=seq(1:nrow(text.df)),
                      text=text.df$Review.Text, 
                      stars=text.df$Review.Star.Rating..out.of.5.) 

```

## Stage one

Cleaning of the data, removing punctuation and if needed, stemming. Data must be stored in 2 versions:
- Version 1: The dataset without stemming and including punctuation (needed for stage three)
- Version 2: The dataset as a complete fully cleaned word vector representation.

```{r}
#Create backup for stage 3 (including punctuation): 
reviews_backup <- reviews 

#First stage of cleaning
reviews$text <- as.character(reviews$text)  %>% 
  tolower() %>% 
  {gsub(":( |-|o)*\\("," SADSMILE ", .)} %>%       # Find :( or :-( or : ( or :o(
  {gsub(":( |-|o)*\\)"," HAPPYSMILE ", .)} %>%     # Find :) or :-) or : ) or :o)
  {gsub("(\"| |\\$)-+\\.-+"," NUMBER", .)} %>%     # Find numbers
  {gsub("([0-9]+:)*[0-9]+ *am"," TIME_AM", .)} %>%         # Find time AM
  {gsub("([0-9]+:)*[0-9]+ *pm"," TIME_PM", .)} %>%         # Find time PM
  {gsub("-+:-+","TIME", .)} %>%                    # Find general time
  {gsub("\\$ ?[0-9]*[\\.,]*[0-9]+"," DOLLARVALUE ", .)} %>%           # Find Dollar values
  {gsub("[0-9]*[\\.,]*[0-9]+"," NUMBER ", .)} %>%           # Find remaining numbers
  {gsub("-"," ", .)} %>%                           # Remove all -
  {gsub("\\("," ", .)} %>%                           #Remove all (
  {gsub("\\)"," ", .)} %>%                           #Remove all )
  {gsub("\""," ", .)} %>%                           #Remove all "
  {gsub("\\,"," ", .)} %>%                            #Remove all ,
  {gsub("&"," and ", .)} %>%                       # Find general time
  {gsub("\"+"," ", .)} %>%                         # Remove all "
  {gsub("\\|+"," ", .)} %>%                        # Remove all |
  {gsub("_+"," ", .)} %>%                          # Remove all _
  {gsub(";+"," ", .)} %>%                          # Remove excess ;
  {gsub(" +"," ", .)} %>%                          # Remove excess spaces
  {gsub("\\.+","\\.", .)}                          # Remove excess .

#stemming, unnesting and removing stop words and create word vector
j <- 1
for (j in 1:nrow(reviews)) {
  stemmed_description <- anti_join((reviews[j,] %>% 
                                      unnest_tokens(word,text, drop=FALSE,to_lower=TRUE) ),stop_words)
  
  stemmed_description <- (wordStem(stemmed_description[,"word"], language = "porter"))
  
  reviews[j,"text"] <- paste((stemmed_description),collapse = " ")
  
}

reviews_vector <- reviews %>% 
                    unnest_tokens(word,text, to_lower=TRUE)

#remove very frequent and infrequent words
counts <- reviews_vector %>% 
            count(word, sort=TRUE)
infrequent <- counts %>% 
                filter(n <0.00005 * nrow(reviews_vector))
frequent <- counts[1:1,] #remove words ski and "c"
toremove <- full_join(frequent,infrequent)
reviews_vector_cleaned <- reviews_vector %>% 
                            anti_join(toremove)

save(reviews_vector_cleaned, file="cleanedwordvector.Rdata") # (2) fully cleaned word vector representation


# Creating the full review from the cleaned and stemmed words
j<-1 
for (j in 1:nrow(reviews)) {
 stemmed_Review<-  anti_join((reviews[j,] %>% 
                                unnest_tokens(word,text,to_lower=TRUE)), toremove)
  
 reviews[j,"text"]<- paste((stemmed_Review[,"word"]),collapse = " ")

}
# Remove empty reviews
reviews <- reviews[!(is.na(reviews$text) | reviews$text==""), ]

save(reviews, file="partially_cleaned_reviews.Rdata") # full sentences
```

##Stage 2: MDS

```{r}
load(file= 'partially_cleaned_reviews.Rdata')

# Creating distance matrix
reviews_corp <- corpus(reviews, docid_field = "ID", text_field = "text")
co_occurrence_matrix <- fcm(x = reviews_corp, context = "document", count = "frequency", tri=FALSE) # get feature co-occurence matrix: number of co-occurences of words

# Moeten we "context = document gebruiken of context = window?" --> JELLE: document omdat we alle woordenaantallen willen pakken in de hele review (https://stackoverflow.com/questions/55509338/is-there-an-r-function-for-finding-keywords-within-a-certain-word-distance)
# Moeten we count = frequency of count = boolean gebruiken? -> JELLE: frequency geeft aan hoe vaak woorden in eenzelfde review bevinden, boolean geeft alleen ja/nee, frequency lijkt mij daarom relevanter

reviews_dfm <- dfm(reviews_corp) # get document frequency matrix: how frequent does each term appear in each document
counts <- colSums(as.matrix(reviews_dfm)) # calculate total count of documents
co_occurrence_matrix <- as.matrix(co_occurrence_matrix)
diag(co_occurrence_matrix) <- counts # adjusts diagonal counts upward (also single counts)

# take a subset of the most frequent words
sortedcount <- counts%>% sort(decreasing=TRUE)
sortednames <- names(sortedcount)
nwords<-200
subset_words<-as.matrix(sortedcount[1:nwords])

# create a subset of the original co_occurance_matrix
co_occurrence_matrix <- co_occurrence_matrix[sortednames[1:nwords],sortednames[1:nwords]]
distances <- sim2diss(co_occurrence_matrix, method = "cooccurrence") # Transform similarities to distances.
```

```{r}

MDS_map <- smacofSym(distances) # run the routine that finds the best matching coordinates in a 2D mp given the distances
ggplot(as.data.frame(MDS_map$conf), aes(D1, D2, label = rownames(MDS_map$conf))) +
     geom_text(check_overlap = TRUE) + theme_minimal(base_size = 15) + xlab('') + ylab('') +
     scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = NULL)
# the conf element in the MDS output contains the coordinatis with as names D1 and D2.
```

##PCA

```{r}
review_tdm <- reviews %>% 
                  unnest_tokens(word,text) %>% 
                      count(word,ID,sort=TRUE) %>%
                        ungroup()%>%
                          cast_tdm(word,ID,n)

# take a subset of the most frequent words
counts <- rowSums(as.matrix(review_tdm)) 
sortedcount <- counts %>% 
                  sort(decreasing=TRUE)
nwords<-200
sortednames <- names(sortedcount[1:nwords])

pca_results <- prcomp(t(review_tdm[,1:10000]), scale = FALSE, rank. = 50) # why do we set scale to FALSE? -> All data is measured on the same scale 
pca_results_backup <- pca_results  # create a backup of results for later use
```

```{r}
fviz_screeplot(pca_results,ncp=30)
```

```{r}
ncomp<-5
# Hoe kies je ncomp?

j<-1
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <- (names(toplist))
for (j in 2:ncomp){
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <-cbind( topwords , (names(toplist)))
  }

topwords

```

```{r warning= FALSE}
#PCA plot
options(ggrepel.max.overlaps= 5)
axeslist <- c(1, 2)
fviz_pca_var(pca_results, axes=axeslist 
             ,geom.var = c("arrow", "text")
              ,col.var = "contrib", # Color by contributions to the PC
              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
              repel = TRUE     # Avoid text overlapping
             )


```

```{r}

rawLoadings     <- pca_results$rotation[sortednames,1:ncomp] %*% diag(pca_results$sdev, ncomp, ncomp)
rotated <- varimax(rawLoadings)
pca_results$rotation <- rotated$loadings
pca_results$x <- scale(pca_results$x[,1:ncomp]) %*% rotated$rotmat


print("PCA finished")

```

```{r}
#select the most important words per rotated dimension

j<-1
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <- (names(toplist))
for (j in 2:ncomp){
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <-cbind( topwords , (names(toplist)))
  }

topwords
```

```{r}
pca_results_small <- pca_results
pca_results_small$x <- pca_results_small$x[1:200,] # Keep only 200 reviews to plot
pca_results <- pca_results_small
```

```{r warning = FALSE}
axeslist <- c(1, 2)
fviz_pca_var(pca_results, axes=axeslist 
             ,geom.var = c("arrow", "text")
              ,col.var = "contrib", # Color by contributions to the PC
              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
              repel = TRUE     # Avoid text overlapping
             )
```

##Sentiment analysis

```{r}
reviews_backup$text <- as.character(reviews_backup$text)  %>%
                 tolower() %>%
                 {gsub(":( |-|o)*\\("," SADSMILE ", .)} %>%       # Find :( or :-( or : ( or :o(
                 {gsub(":( |-|o)*\\)"," HAPPYSMILE ", .)} %>%     # Find :) or :-) or : ) or :o)
                 {gsub("\\n", " ", .)} %>%                    # Remove \n (newline)     
                 {gsub("[?!]+", ".", .)} %>%                  # Remove ? and ! (replace by single .)
                 {gsub("[\\[\\*\\]]*", " ", .)} %>%           # Remove [ and ] * (replace by single space)
                 {gsub("(\"| |\\$)-+\\.-+", " number ", .)} %>%# Find numbers
                 {gsub("(-+:)*-+ *am", " timeam", .)} %>%     # Find time AM
                 {gsub("(-+:)*-+ *pm", " timepm", .)} %>%     # Find time PM
                 {gsub("-+:-+", "time", .)} %>%               # Find general time
                 {gsub("( |\\$)--+", " number ", .)} %>%      # Find remaining numbers
                 {gsub("-"," ", .)} %>%                       # Remove all -
                 {gsub("\"+", " ", .)} %>%                    # Remove all "
                 {gsub(";+", " ", .)} %>%                     # Remove excess ;
                 {gsub("\\.+","\\. ", .)} %>%                 # Remove excess .
                 {gsub(" +"," ", .)} %>%                      # Remove excess spaces
                 {gsub("\\. \\.","\\. ", .)}                  # Remove space between periods

#create subset for testing
reviews_test <- reviews_backup[1:500,]
                           
all_words <- reviews_test[,] %>%
             unnest_tokens("text", output = "word") %>%
             anti_join(stop_words, by = "word") %>%
             count(word, sort = TRUE) %>%
             filter(n>50)
sentiment_scores <- polarity(all_words$word)$all        # get sentiment with polarity function
all_words$sentiment <- sentiment_scores[,"polarity"]

# Create plot
all_words %>%
#  filter(n > 1500) %>%
  filter(sentiment != 0) %>%
  mutate(n = ifelse(sentiment == -1, -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  mutate(Sentiment = ifelse(sentiment == 1, "Postive","Negative")) %>%
  ggplot(aes(word, n, fill = Sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to \"total\" sentiment", x = "Word (min freq = 50)")
```

```{r}
pol <- polarity(reviews_test[, "text"])$all
# Set negators and amplifiers on empty to see how it influences polarity score
pol2 <- polarity(reviews_test[,'text'], negators = vector(), amplifiers = vector(), )$all
# Standard uses the dictionary of qdap, here it is empty. Maybe use a dictionary of another source to compare.

reviews_test$polarity <- pol[, "polarity"]
reviews_test$polarity2 <- pol2[,"polarity"]
#remove empty reviews
reviews_test<-reviews_df[!(reviews_df$polarity=="NaN"),]

```

```{r}
# Split reviews
pos_reviews      <- reviews_test %>% filter(stars >= 4)
neg_reviews      <- reviews_test %>% filter(stars < 4) #change to < 3?

#get mean values of bing and polarity 
for (v in c("sent_bing","polarity","polarity2")) {
  cat(v)
  cat(" ", mean(neg_reviews[, v]), " ", mean(pos_reviews[, v]), "\n")
}
```

```{r}
#Polarity plots
ggplot( ,aes(polarity)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill")

ggplot( ,aes(polarity)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill")

```

```{r}
#Polarity plots with negators and amplifiers on 0 
ggplot( ,aes(polarity2)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill")
```

