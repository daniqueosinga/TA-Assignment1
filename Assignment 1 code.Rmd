---
title: "Assignment 1 ski data"
author: "Larissa, Ralph, Danique en Jelle"
date: "3/22/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set options
options(stringsAsFactors=F)
Sys.setlocale('LC_ALL','C')

# Loading libraries
library(stringi)
library(stringr)
library(qdap)
library(lubridate)
library(tm)
library(ggplot2)
library(wordcloud)
library(jsonlite)
library(dplyr)
library(tokenizers)
library(tidytext)
library(SnowballC)
library(corpus)
library(plotrix)
library(mgsub)
library(syuzhet)
library(ggrepel)
library(quanteda)
library(smacof)
library(factoextra)
library(readr)

# Loading data files
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

text.df <- read.csv("OnTheSnow_SkiAreaReviews.csv", comment.char="#")

reviews <- data.frame(ID=seq(1:nrow(text.df)),
                      text=text.df$Review.Text, 
                      stars=text.df$Review.Star.Rating..out.of.5.) 
#Create backup for stage 3 (including punctuation): 
reviews_backup <- reviews 
```

## Stage one

Cleaning of the data, removing punctuation and if needed, stemming. Data must be stored in 2 versions:
- Version 1: The dataset without stemming and including punctuation (needed for stage three)
- Version 2: The dataset as a complete fully cleaned word vector representation.

```{r, warning = FALSE, message = FALSE}

# Deze kan je overslaan als je de chunk hieronder de saved files inlaad

#First stage of cleaning
reviews$text <- as.character(reviews$text)  %>% 
  tolower() %>% 
  {gsub(":( |-|o)*\\("," SADSMILE ", .)} %>%       # Find :( or :-( or : ( or :o(
  {gsub(":( |-|o)*\\)"," HAPPYSMILE ", .)} %>%     # Find :) or :-) or : ) or :o)
  {gsub("(\"| |\\$)-+\\.-+"," NUMBER", .)} %>%     # Find numbers
  {gsub("([0-9]+:)*[0-9]+ *am"," TIME_AM", .)} %>%         # Find time AM
  {gsub("([0-9]+:)*[0-9]+ *pm"," TIME_PM", .)} %>%         # Find time PM
  {gsub("-+:-+","TIME", .)} %>%                    # Find general time
  {gsub("\\$ ?[0-9]*[\\.,]*[0-9]+"," DOLLARVALUE ", .)} %>%           # Find Dollar values
  {gsub("[0-9]*[\\.,]*[0-9]+"," NUMBER ", .)} %>%           # Find remaining numbers
  {gsub("-"," ", .)} %>%                           # Remove all -
  {gsub("\\("," ", .)} %>%                           #Remove all (
  {gsub("\\)"," ", .)} %>%                           #Remove all )
  {gsub("\""," ", .)} %>%                           #Remove all "
  {gsub("\\,"," ", .)} %>%                            #Remove all ,
  {gsub("&"," and ", .)} %>%                       # Find general time
  {gsub("\"+"," ", .)} %>%                         # Remove all "
  {gsub("\\|+"," ", .)} %>%                        # Remove all |
  {gsub("_+"," ", .)} %>%                          # Remove all _
  {gsub(";+"," ", .)} %>%                          # Remove excess ;
  {gsub(" +"," ", .)} %>%                          # Remove excess spaces
  {gsub("\\.+","\\.", .)}                          # Remove excess .

#stemming, unnesting and removing stop words and create word vector
j <- 1
for (j in 1:nrow(reviews)) {
  stemmed_description <- anti_join((reviews[j,] %>% 
                                      unnest_tokens(word,text, drop=FALSE,to_lower=TRUE) ),stop_words)
  
  stemmed_description <- (wordStem(stemmed_description[,"word"], language = "porter"))
  
  reviews[j,"text"] <- paste((stemmed_description),collapse = " ")
  
}

reviews_vector <- reviews %>% 
                    unnest_tokens(word,text, to_lower=TRUE)

#remove very frequent and infrequent words
counts <- reviews_vector %>% 
            count(word, sort=TRUE)
infrequent <- counts %>% 
                filter(n <0.00005 * nrow(reviews_vector))
frequent <- counts[1:1,] #remove words ski and "c"
toremove <- full_join(frequent,infrequent)
reviews_vector_cleaned <- reviews_vector %>% 
                            anti_join(toremove)

#save(reviews_vector_cleaned, file="cleanedwordvector.Rdata") # (2) fully cleaned word vector representation


# Creating the full review from the cleaned and stemmed words
j<-1 
for (j in 1:nrow(reviews)) {
 stemmed_Review<-  anti_join((reviews[j,] %>% 
                                unnest_tokens(word,text,to_lower=TRUE)), toremove)
  
 reviews[j,"text"]<- paste((stemmed_Review[,"word"]),collapse = " ")

}
# Remove empty reviews
reviews <- reviews[!(is.na(reviews$text) | reviews$text==""), ]

#save(reviews, file="partially_cleaned_reviews.Rdata") # full sentences
```

##Stage 2: MDS

```{r}
load(file= 'partially_cleaned_reviews.Rdata')
load(file = "cleanedwordvector.Rdata")

# Creating distance matrix
reviews_corp <- corpus(reviews, docid_field = "ID", text_field = "text")
co_occurrence_matrix <- fcm(x = reviews_corp, context = "window", count = "frequency", tri=FALSE, window = 5) # get feature co-occurence matrix: number of co-occurences of words

# Moeten we "context = document gebruiken of context = window?" --> JELLE: document omdat we alle woordenaantallen willen pakken in de hele review (https://stackoverflow.com/questions/55509338/is-there-an-r-function-for-finding-keywords-within-a-certain-word-distance)
# Moeten we count = frequency of count = boolean gebruiken? -> JELLE: frequency geeft aan hoe vaak woorden in eenzelfde review bevinden, boolean geeft alleen ja/nee, frequency lijkt mij daarom relevanter

reviews_dfm <- dfm(reviews_corp) # get document frequency matrix: how frequent does each term appear in each document
counts <- colSums(as.matrix(reviews_dfm)) # calculate total count of documents
co_occurrence_matrix <- as.matrix(co_occurrence_matrix)
diag(co_occurrence_matrix) <- counts # adjusts diagonal counts upward (also single counts)

# take a subset of the most frequent words
sortedcount <- counts%>% sort(decreasing=TRUE)
sortednames <- names(sortedcount)
nwords<-200
subset_words<-as.matrix(sortedcount[1:nwords])

# create a subset of the original co_occurance_matrix
co_occurrence_matrix <- co_occurrence_matrix[sortednames[1:nwords],sortednames[1:nwords]]
distances <- sim2diss(co_occurrence_matrix, method = "cooccurrence") # Transform similarities to distances.
```

```{r}

MDS_map <- smacofSym(distances) # run the routine that finds the best matching coordinates in a 2D mp given the distances
ggplot(as.data.frame(MDS_map$conf), aes(D1, D2, label = rownames(MDS_map$conf))) +
     geom_text(check_overlap = TRUE) + theme_minimal(base_size = 15) + xlab('') + ylab('') +
     scale_y_continuous(breaks = NULL) + scale_x_continuous(breaks = NULL)
# the conf element in the MDS output contains the coordinatis with as names D1 and D2.
```

##PCA

```{r}
#Deze chunk kan je overslaan als je de volgende chunk gebruikt om de PCA results in te laden

review_tdm <- reviews %>% 
                  unnest_tokens(word,text) %>% 
                      count(word,ID,sort=TRUE) %>%
                        ungroup()%>%
                          cast_tdm(word,ID,n)

# take a subset of the most frequent words
counts <- rowSums(as.matrix(review_tdm)) 
sortedcount <- counts %>% 
                  sort(decreasing=TRUE)
nwords<-200
sortednames <- names(sortedcount[1:nwords])

pca_results <- prcomp(t(review_tdm[,1:10000]), scale = FALSE, rank. = 50) # why do we set scale to FALSE? -> All data is measured on the same scale 
pca_results_backup <- pca_results  # create a backup of results for later use

#Save pca results
#saveRDS(pca_results_backup,'pca_results.rds')

```

```{r}
#load(file = "pca_results.Rdata")
pca_results <- readRDS(file = "pca_results.rds")


#pca_results_backup <- pca_results
fviz_screeplot(pca_results,ncp=30)
```

```{r}
ncomp<-2
# Hoe kies je ncomp?

j<-1
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <- (names(toplist))
for (j in 2:ncomp){
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <-cbind( topwords , (names(toplist)))
  }

topwords

```

```{r warning= FALSE}
#PCA plot
options(ggrepel.max.overlaps= 5)
axeslist <- c(1, 2)
fviz_pca_var(pca_results, axes=axeslist 
             ,geom.var = c("arrow", "text")
              ,col.var = "contrib", # Color by contributions to the PC
              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
              repel = TRUE,# Avoid text overlapping
             select.var = list(contrib = 10) 
             )


```

```{r}

rawLoadings <- pca_results$rotation[sortednames,1:ncomp] %*% diag(pca_results$sdev, ncomp, ncomp)
rotated <- varimax(rawLoadings)
pca_results$rotation <- rotated$loadings
pca_results$x <- scale(pca_results$x[,1:ncomp]) %*% rotated$rotmat


print("PCA finished")

```

```{r}
#select the most important words per rotated dimension

j<-1
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <- (names(toplist))
for (j in 2:ncomp){
toplist <- abs(pca_results$rotation[,j]) %>% sort(decreasing=TRUE) %>% head(10)
topwords <-cbind( topwords , (names(toplist)))
  }

topwords
```

```{r}
pca_results_small <- pca_results
pca_results_small$x <- pca_results_small$x[1:200,] # Keep only 200 reviews to plot
pca_results <- pca_results_small
```

```{r warning = FALSE}
axeslist <- c(1, 2)
fviz_pca_var(pca_results, axes=axeslist 
             ,geom.var = c("arrow", "text")
              ,col.var = "contrib", # Color by contributions to the PC
              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
              repel = TRUE,     # Avoid text overlapping,
             select.var = list(contrib = 10) 
             )
```

##Sentiment analysis

```{r}
reviews_sentiment <- reviews_backup
reviews_sentiment$text <- as.character(reviews_sentiment$text)  %>%
                 tolower() %>%
                 {gsub(":( |-|o)*\\("," SADSMILE ", .)} %>%       # Find :( or :-( or : ( or :o(
                 {gsub(":( |-|o)*\\)"," HAPPYSMILE ", .)} %>%     # Find :) or :-) or : ) or :o)
                 {gsub("\\n", " ", .)} %>%                    # Remove \n (newline)     
                 {gsub("[?!]+", ".", .)} %>%                  # Remove ? and ! (replace by single .)
                 {gsub("[\\[\\*\\]]*", " ", .)} %>%           # Remove [ and ] * (replace by single space)
                 {gsub("(\"| |\\$)-+\\.-+", " number ", .)} %>%# Find numbers
                 {gsub("(-+:)*-+ *am", " timeam", .)} %>%     # Find time AM
                 {gsub("(-+:)*-+ *pm", " timepm", .)} %>%     # Find time PM
                 {gsub("-+:-+", "time", .)} %>%               # Find general time
                 {gsub("( |\\$)--+", " number ", .)} %>%      # Find remaining numbers
                 {gsub("-"," ", .)} %>%                       # Remove all -
                 {gsub("\"+", " ", .)} %>%                    # Remove all "
                 {gsub(";+", " ", .)} %>%                     # Remove excess ;
                 {gsub("\\.+","\\. ", .)} %>%                 # Remove excess .
                 {gsub(" +"," ", .)} %>%                      # Remove excess spaces
                 {gsub("\\. \\.","\\. ", .)}                  # Remove space between periods

#create subset for testing
#reviews_test <- reviews_sentiment[1:500,]

0.05*nrow(reviews_sentiment)
                           
all_words <- reviews_sentiment[,] %>%
             unnest_tokens("text", output = "word") %>%
             anti_join(stop_words, by = "word") %>%
             count(word, sort = TRUE) %>%
             filter(n> 0.05*nrow(reviews_sentiment))
sentiment_scores <- polarity(all_words$word)$all        # get sentiment with polarity function
all_words$sentiment <- sentiment_scores[,"polarity"]

# Create plot
all_words %>%
#  filter(n > 1500) %>%
  filter(sentiment != 0) %>%
  mutate(n = ifelse(sentiment == -1, -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  mutate(Sentiment = ifelse(sentiment == 1, "Postive","Negative")) %>%
  ggplot(aes(word, n, fill = Sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to \"total\" sentiment", x = "Word (min freq = 5% of observations)")
```

```{r}

#Deze functie duurt heel lang. Dus je kan beter in de chunk onder deze de data loaden.

pol <- polarity(reviews_sentiment[, "text"])$all
# Set negators and amplifiers on empty to see how it influences polarity score
pol2 <- polarity(reviews_sentiment[,'text'], negators = vector(), amplifiers = vector(), )$all
# Set negators on empty, set amplifier normal
pol3 <- polarity(reviews_sentiment[,'text'], negators = vector(), amplifiers = qdapDictionaries::amplification.words, )$all
# Set negators normal, amplification empty
pol4 <- polarity(reviews_sentiment[,'text'], negators = qdapDictionaries::negation.words, amplifiers = vector(), )$all



reviews_sentiment$polarity <- pol[, "polarity"]
reviews_sentiment$polarity2 <- pol2[,"polarity"]
reviews_sentiment$polarity3 <- pol3[,"polarity"]
reviews_sentiment$polarity4 <- pol4[,"polarity"]
#remove empty reviews
reviews_sentiment<-reviews_sentiment[!(reviews_sentiment$polarity=="NaN"),]


#Save with polarity
#save(reviews_sentiment, file="reviews_sentiment_with_polarity.Rdata")


```

```{r}
#load polarity data
load(file= 'reviews_sentiment_with_polarity.Rdata')



# Split reviews
pos_reviews      <- reviews_sentiment %>% filter(stars >= 4)
neg_reviews      <- reviews_sentiment %>% filter(stars < 3) #neutral reviews not taken into account

#get mean values of bing and polarity 
mean(pos_reviews$polarity, na.rm = TRUE)
mean(neg_reviews$polarity, na.rm = TRUE)
mean(pos_reviews$polarity2, na.rm = TRUE)
mean(neg_reviews$polarity2, na.rm = TRUE)
mean(pos_reviews$polarity3, na.rm = TRUE)
mean(neg_reviews$polarity3, na.rm = TRUE)
mean(pos_reviews$polarity4, na.rm = TRUE)
mean(neg_reviews$polarity4, na.rm = TRUE)

```

```{r}
#Polarity plots
ggplot( ,aes(polarity)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill") +
  ggtitle("Polarity histogram with negators and with amplifiers")

ggplot( ,aes(polarity)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill") +
  ggtitle("Polarity histogram with negators and with amplifiers")

```

```{r}
#Polarity plots with negators and amplifiers on 0 
ggplot( ,aes(polarity2)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill") + 
  ggtitle("Polarity histogram with no negators and no amplifiers") + labs(x = "polarity")

ggplot( ,aes(polarity2)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill")+ 
  ggtitle("Polarity histogram with no negators and no amplifiers") + labs(x = "polarity")
```

```{r}
#Polarity plots with negators and amplifiers on 0 
ggplot( ,aes(polarity3)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill") + 
  ggtitle("Polarity histogram with no negators but with amplifiers")+ labs(x = "polarity")

ggplot( ,aes(polarity3)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill")+ 
  ggtitle("Polarity histogram with no negators but with amplifiers")+ labs(x = "polarity")
```

```{r}
#Polarity plots with negators and amplifiers on 0 
ggplot( ,aes(polarity4)) +
    geom_histogram(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_histogram(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill") + 
  ggtitle("Polarity histogram with negators but with no amplifiers")+ labs(x = "polarity")

ggplot( ,aes(polarity4)) +
    geom_density(aes(fill = "Happy"),   data = pos_reviews, alpha = 0.5) +
    geom_density(aes(fill = "Unhappy"), data = neg_reviews, alpha = 0.5) +
    scale_colour_manual("Polarity", values = c("green", "red"), aesthetics = "fill")+ 
  ggtitle("Polarity histogram with negators but with no amplifiers")+ labs(x = "polarity")
```



##LDA analysis
```{r, LDA}
library(topicmodels)


reviews_lda <- reviews_backup
reviews_lda$text <- as.character(reviews_lda$text)  %>%
  tolower() %>%
  {gsub("\\n", " ", .)} %>%                        # Remove \n (newline)     
  {gsub("[?!]+",".",.)} %>%                        # Remove ? and ! (replace by single .)
  {gsub("[\\[\\*\\]]*"," ",.)} %>%                 # Remove [ and ] * (replace by single space)
  {gsub("(\"| |\\$)-+\\.-+"," number ", .)} %>%    # Find numbers
  {gsub("(-+:)*-+ *am"," timeam", .)} %>%          # Find time AM
  {gsub("(-+:)*-+ *pm"," timepm", .)} %>%          # Find time PM
  {gsub("-+:-+","time", .)} %>%                    # Find general time
  {gsub("( |\\$)--+"," number ", .)} %>%           # Find remaining numbers
  {gsub("-"," ", .)} %>%                           # Remove all -
  {gsub("\"+"," ", .)} %>%                         # Remove all "
  {gsub(";+"," ", .)} %>%                          # Remove excess ;
  {gsub("\\.+","\\. ", .)} %>%                     # Remove excess .
  {gsub(" +"," ", .)} %>%                          # Remove excess spaces
  {gsub("\\. \\.","\\. ", .)}                      # Remove space between periods

index <- sample(1:nrow(reviews_lda), 2000)
reviews_lda_subset <- reviews_backup[index,]
#save(reviews_lda_subset, file="reviews_lda_subset.Rdata")


load(file = 'reviews_lda_subset.Rdata')

#create dtm 
corpus <- Corpus(VectorSource(unlist(reviews_lda_subset[, "text"])))
dtm <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, stopwords = TRUE, 
                                                 wordLengths = c(3,20), removeNumbers = TRUE, 
                                                 removePunctuation = TRUE))

#Remove words that do not appear often enough (next remove documents that may now be empty)
library(slam)
term_count <- col_sums(dtm > 0)
term_perc <- term_count / nrow(dtm)

lowFreq <- (col_sums(dtm)<1)
dtm <- dtm[, !lowFreq]
dtm <- dtm[row_sums(dtm) > 0,]
summary(col_sums(dtm))
n <- nrow(dtm)
dim(dtm)

#split into validation (20%) and train (80%)
set.seed(seed=2020)
train <- sample(1:n, round(n * 0.80))
valid <- (1:n)[-train]
```

```{r}
#Estimate LDA for different values of K and collect perplexity values 
#Use perplexity to determine value of alpha
#Loow perplexity is good 

#Deze duurt lang. Run de chunk die de alpha inlaadt

res <- NULL
TMlist <- list()  # list to collect results

for (l in seq(10, 50, 10)) {
  print(l)
  
  TM <- LDA(dtm[train, ], method="Gibbs", k = l) #, control = list(alpha = 0.5))
  
  p       <- perplexity(TM, newdata = dtm[train,])
  p_valid <- perplexity(TM, newdata = dtm[valid, ])
  
  res <- rbind(res, data.frame(k=l, perplexity = p, perplexity_validation = p_valid))
  TMlist <- append(TMlist, TM)
}
print(res)
```

Lowest perplexity at K = 30

```{r}
#Plot perplexity 
ggplot(res, aes(k)) +                    
  geom_line(aes(y=perplexity, colour="Train")) +
  geom_line(aes(y=perplexity_validation, colour="Validation")) + labs(colour="Sample")

# Print results with best perplexity
res[which.min(res$perplexity_validation),]
#both train and validation lowest at K = 30 

#estimate for different values of alpha at K = 30
k <- 30
SEED <- 2020
resAlpha <- NULL
TMlistAlpha <- list()

for (a in seq(.5, 3, .5))
{
  print(a)
  
  TMAlpha <- LDA(dtm[train, ], method="Gibbs", k = k, control = list(alpha = a))
  
  p <- perplexity(TMAlpha, newdata = dtm[train,])
  p_valid <- perplexity(TMAlpha, newdata = dtm[valid, ])
  
  resAlpha <- rbind(resAlpha, data.frame(alpha=a, perplexity = p, perplexity_validation = p_valid))
  TMlistAlpha <- append(TMlistAlpha, TMAlpha)
}
print(resAlpha)
#perplexity lowest at alpha = ?  

#few more alpha test
for (a in seq(0.1, 0.4, .1))
{
  print(a)
  
  TMAlpha <- LDA(dtm[train, ], method="Gibbs", k = k, control = list(alpha = a))
  
  p <- perplexity(TMAlpha, newdata = dtm[train,])
  p_valid <- perplexity(TMAlpha, newdata = dtm[valid, ])
  
  resAlpha <- rbind(resAlpha, data.frame(alpha=a, perplexity = p, perplexity_validation = p_valid))
  TMlistAlpha <- append(TMlistAlpha, TMAlpha)
}
print(resAlpha)

#plot perplexity vs alpha
ggplot(resAlpha, aes(alpha)) +                    
  geom_line(aes(y=perplexity, colour="Train")) +   
  geom_line(aes(y=perplexity_validation, colour="Validation"))  +
  labs(colour="Sample")

# Show best alpha
resAlpha[which.min(resAlpha$perplexity_validation),]
#alpha = 0.3

#save(resAlpha, file = 'resAlpha.Rdata')

```

```{r}

#Vanaf hier LDA inladen
library(topicmodels)
load(file = 'resAlpha.Rdata')

#so K = 30, alpha = 0.3
alpha = resAlpha[which.min(resAlpha$perplexity_validation),]$alpha

TM <- LDA(dtm[train, ], k = k, method="Gibbs", control = list(seed = SEED, alpha=alpha))
```


```{r, LDA visualization}
#visualize assignment to topics
posterior(TM)$topics[1:15,]
maxProb <- apply(posterior(TM)$topics, 1, max)
hist(maxProb, main="Assignment to topics", xlab="Maximum topic probability")

#Print term probabilities per topic
p <- posterior(TM)$terms
p[,1:8]

#Show the "size" of each topic
col_means(posterior(TM)$topics)

#Most likely topic per document
t <- topics(TM, 1)  
as.matrix(t)[1:4] # show the first 4

#Most likely terms per topic
Terms <- terms(TM, 5)
Terms[,1:5]

#Graphically show most likely terms
text_topics <- tidy(TM, matrix = "beta")
text_topics

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

perplot <- 6
for (i in 1:ceiling(k/perplot))
{
  p <- text_top_terms %>%
    filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()+
    scale_x_reordered()
  show(p)
}
```
